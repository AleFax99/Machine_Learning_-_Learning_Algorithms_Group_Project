---
title: "ML_project_Random_Forest_09.12.2023"
output: html_document
date: "2023-12-09"
---

# Random forest model 

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

Loading libraries:

```{r}
# install.packages("ranger")
library(ranger)
```


## Preprocessing recipe


```{r}
rf_recipe <- RSM_train %>% 
  recipe(Status ~. , data = RSM_train) %>% 
  update_role(AppDate, AppYear, OfferDate, ResponseDate, new_role = "metadata")

rf_recipe
```

Tuning:
```{r}
rf_tune <- rand_forest(mtry = tune(), trees = 1000) |>
  set_mode("classification") |>
  set_engine("ranger", importance = "permutation")

rf_tune
```

Setting up the workflow:
```{r}
rf_tune_wf <-
  workflow() |>
  add_recipe(rf_recipe) |>
  add_model(rf_tune)
rf_tune_wf

```
## Defining the metrics for the model. 

```{r}
class_metrics <- metric_set(
  accuracy, sensitivity,
  specificity, roc_auc, kap
)
```

```{r}
rf_tune_grid <- grid_regular(mtry(range = c(1, 14)), levels = 14)

```


```{r}
set.seed(4433553)
rf_tune_res <- tune_grid(
  rf_tune_wf,
  resamples = cv_folds,
  grid = rf_tune_grid,
  metrics = class_metrics
)
```

Collecting the metrics:
```{r}
rf_metrics <-
  rf_tune_res |>
  collect_metrics()
rf_metrics
```


Plotting the results of the metrics:
```{r}
rf_tune_res |>
  collect_metrics() |>
  filter(.metric %in% c("accuracy", "roc_auc")) |>
  ggplot(aes(
    x = mtry, y = mean, ymin = mean - std_err,
    ymax = mean + std_err,
    colour = .metric
  )) +
  geom_errorbar() +
  geom_line() +
  geom_point() +
  scale_colour_manual(values = c("#D55E00", "#0072B2")) +
  facet_wrap(~.metric, ncol = 1, scales = "free_y") +
  guides(colour = "none") +
  theme_bw()
```

```{r}
rf_tune_res |>
  collect_metrics() |>
  filter(.metric %in% c("sensitivity", "specificity")) |>
  ggplot(aes(
    x = mtry, y = mean, ymin = mean - std_err,
    ymax = mean + std_err,
    colour = .metric
  )) +
  geom_errorbar() +
  geom_line() +
  geom_point() +
  scale_colour_manual(values = c("black", "green")) +
  facet_wrap(~.metric, ncol = 1, scales = "free_y") +
  guides(colour = "none") +
  theme_bw()
```
As specified in the data pre-processing part, we care most about accuracy of the model. However, we decided to plot sensitivity, specificity and area under the curve in order to make sure we don't overly sacrifice one metric to achieve higher performance in the other. Looking at the graphs, accuracy seems to be maximised for the values between 2 and 7. At mtry = 1, Sensitivity reaches its maximum, whereas specificity is at its minimum. To correct for that, choosing values between 2 and 7 allows us to sacrifice a bit of sensitivity to achieve better specificity. For the low values of mtry, we also achieve high values of area under the curve. 

Choosing the best model according to the 1SE rule - here the lower number of mtry suggests a simpler model, since the algorithm has to consider less variables at each split. Choosing a lower mtry also lowers the risk of overfitting. Given consistent high metric values over the range 2-7, choosing 2 seems like a reasonable choice. 
```{r}
best_rf <- select_by_one_std_err(rf_tune_res, metric = "accuracy", mtry)
best_rf
```
Accuracy with mtry of 2 seems to confirm our previous reasoning, where at the same time we are able to achieve both high sensitivity and specificity, as well as reasonable area under the curve measure. 

Finalizing the workflow:
```{r}
rf_final_wf <- finalize_workflow(rf_tune_wf, best_rf)
rf_final_wf
```
## Test set performance on 2023 data

```{r}
set.seed(34662947) #Call me 
rf_final_fit <- 
  rf_final_wf %>% 
  last_fit(analysis_assessment_split, metrics = class_metrics)
  
```

Collecting metrics:
```{r}
rf_final_fit %>% 
  collect_metrics()
```
The model that was chosen, has an accuracy score of 96.1%, slightly lower than the training estimation (97.0%). Moreover, The model performs worse than expected in terms of sensitivity, and better than expected for specificity. The area under the curve achieves a comparable value to the one in training.   


